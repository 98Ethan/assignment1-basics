# Basic training run (small GPTs)
  uv run python train.py \
    --train_data ./data/tokens/TinyStoriesV2-GPT4-train.bin \
    --val_data ./data/tokens/TinyStoriesV2-GPT4-valid.bin \
    --vocab_size 10000 \
    --context_length 256 \
    --d_model 512 \
    --num_layers 4 \
    --num_heads 16 \
    --d_ff 1344
    --batch_size 512 \
    --max_iters 2500 \
    --eval_interval 250 \
    --eval_iters 100 \ 
    --warmup_iters 150 \
    --cosine_cycle_iters 2374 \
    --lr_max 6e-4 \
    --lr_min 6e-5 \
    --beta2 0.95 \
    --weight_decay 0.05 \
    --wandb \
    --wandb_project tinystories_1


  # Smaller model for quick testing
  uv run python train.py \
    --train_data ./data/tokens/TinyStoriesV2-GPT4-train.bin \
    --val_data ./data/tokens/TinyStoriesV2-GPT4-train.bin \
    --vocab_size 32000 \
    --context_length 128 \
    --batch_size 8 \
    --max_iters 1000 \
    --d_model 256 \
    --num_layers 4 \
    --num_heads 4 \
    --d_ff 1024 \
    --eval_interval 100 \
    --log_interval 50

  # With W&B logging and custom checkpoint directory
  uv run python train.py \
    --train_data ./data/tokens/TinyStoriesV2-GPT4-train.bin \
    --val_data ./data/tokens/TinyStoriesV2-GPT4-train.bin \
    --checkpoint_dir ./my_checkpoints \
    --wandb \
    --wandb_project my-transformer-experiment